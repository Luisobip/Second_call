{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370db117",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5cdcf8",
   "metadata": {},
   "source": [
    "Create a project with code that solves the following questions. Please generate a repository in GitHub with the code and make all necessary commits. The use of any AI tool is forbidden.\n",
    "\n",
    "1. (4 points) Generate a pipeline for processing a text. The pipeline will have the following porcesses: Sentence splitter, word tokenization, conver acronims with dots to acronims whitout dots with regex (U.S.A. to USA), remove stopwords, lematizer. You must do each step in pure python and with a NLP library of your choice.\n",
    "\n",
    "2. (4 points) Create an application with user interface that call a local LLM to solve any problem you choose. Modify the system prompt to better solve the problem.\n",
    "\n",
    "3. (2 points) Integrate in the application of the previous question with a speech model for speech to text and text to speech. The model should be from Huging Face.\n",
    "\n",
    "Please upload the code files and a link to the github repository. The assesment will be with a one on one defense. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d767e",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Genarate a pipeline for processing a text.\n",
    "\n",
    "the text I will be using to complete this task is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ccbdcb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The president of the U.S.A., Donald Trump, is 1.9m high and 78 years old. Forbes Magazine has assessed his wealth, currently estimating it at $5.5 billion as of mid-February 2025.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a9f9e0",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f807b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd71ac3",
   "metadata": {},
   "source": [
    "Using NLTK to split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4179f4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The president of the U.S.A., Donald Trump, is 1.9m high and 78 years old.', 'Forbes Magazine has assessed his wealth, currently estimating it at $5.5 billion as of mid-February 2025.']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf935ca1",
   "metadata": {},
   "source": [
    "Using NLTK for word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16afd385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'president', 'of', 'the', 'U.S.A.', ',', 'Donald', 'Trump', ',', 'is', '1.9m', 'high', 'and', '78', 'years', 'old', '.', 'Forbes', 'Magazine', 'has', 'assessed', 'his', 'wealth', ',', 'currently', 'estimating', 'it', 'at', '$', '5.5', 'billion', 'as', 'of', 'mid-February', '2025', '.']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7b93ab",
   "metadata": {},
   "source": [
    "Convert acronyms with regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47306271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president of the USA, Donald Trump, is 190 centimeters high and 78 years old. Forbes Magazine has assessed his wealth, currently estimating it at five point five billion as of mid-February 2025.\n"
     ]
    }
   ],
   "source": [
    "# For U:S:A.\n",
    "text = re.sub(r'U\\.S\\.A\\.', 'USA', text)\n",
    "\n",
    "# For numbers of a height and money\n",
    "\n",
    "text = re.sub(r'(\\d+\\.\\d+)m', lambda x: f\"{int(float(x.group(1)) * 100)} centimeters\", text)\n",
    "\n",
    "def decimal_to_words(match):\n",
    "    number = match.group(1)  # Extract the decimal number (\"5.5\")\n",
    "    integer_part, decimal_part = number.split(\".\")  # Split into whole and decimal parts\n",
    "    return f\"{num2words(int(integer_part))} point {num2words(int(decimal_part))} billion\"\n",
    "\n",
    "text = re.sub(r'\\$(\\d+\\.\\d+)\\sbillion', decimal_to_words, text)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481c3a7",
   "metadata": {},
   "source": [
    "remove stopwords using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f8a2b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the president of the USA, Donald_Trump, is 190 centimeters high and 78 years old. Forbes_Magazine has assessed his wealth, currently estimating it at five point five billion as of mid-February 2025.\n"
     ]
    }
   ],
   "source": [
    "# Convert to lower case (except proper nouns)\n",
    "def process_text(text):\n",
    "    proper_nouns = [\"USA\", \"Donald Trump\", \"Forbes Magazine\", \"mid-February\"]\n",
    "    \n",
    "    for proper in proper_nouns:\n",
    "        text = re.sub(r'\\b' + re.escape(proper) + r'\\b', proper.replace(\" \", \"_\"), text)\n",
    "\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if \"_\" in word or word.isupper() or word == \"mid-February\":  \n",
    "            processed_words.append(word)\n",
    "        else:\n",
    "            processed_words.append(word.casefold())\n",
    "\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "processed_text = process_text(text)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7684bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the president of the USA, Donald_Trump, is 190 centimeters high and 78 years old.', 'Forbes_Magazine has assessed his wealth, currently estimating it at five point five billion as of mid-February 2025.']\n",
      "['the', 'president', 'of', 'the', 'USA', ',', 'Donald_Trump', ',', 'is', '190', 'centimeters', 'high', 'and', '78', 'years', 'old', '.', 'Forbes_Magazine', 'has', 'assessed', 'his', 'wealth', ',', 'currently', 'estimating', 'it', 'at', 'five', 'point', 'five', 'billion', 'as', 'of', 'mid-February', '2025', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the processed text\n",
    "processed_words = nltk.word_tokenize(processed_text)\n",
    "processed_sentences = nltk.sent_tokenize(processed_text)\n",
    "print(processed_sentences)\n",
    "print(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad64138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['president', 'USA', ',', 'Donald_Trump', ',', '190', 'centimeters', 'high', '78', 'years', 'old', '.', 'Forbes_Magazine', 'assessed', 'wealth', ',', 'currently', 'estimating', 'five', 'point', 'five', 'billion', 'mid-February', '2025', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "\n",
    "def removestopwords(tokens):\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))  # Get the stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "filtered_words = removestopwords(processed_words)\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723ac9e8",
   "metadata": {},
   "source": [
    "Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186e9266",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m lemmatizer = WordNetLemmatizer()\n\u001b[32m      8\u001b[39m ignore_letters = [\u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m!\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m lemma_w = \u001b[43m[\u001b[49m\u001b[43mlemmatizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_words\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mignore_letters\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_wordnet_pos\u001b[39m(tag):\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tag.startswith(\u001b[33m'\u001b[39m\u001b[33mJ\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      6\u001b[39m lemmatizer = WordNetLemmatizer()\n\u001b[32m      8\u001b[39m ignore_letters = [\u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m!\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m lemma_w = [\u001b[43mlemmatizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_words\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ignore_letters]\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_wordnet_pos\u001b[39m(tag):\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tag.startswith(\u001b[33m'\u001b[39m\u001b[33mJ\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luiss\\miniconda3\\envs\\NLP\\Lib\\site-packages\\nltk\\stem\\wordnet.py:85\u001b[39m, in \u001b[36mWordNetLemmatizer.lemmatize\u001b[39m\u001b[34m(self, word, pos)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     61\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Lemmatize `word` by picking the shortest of the possible lemmas,\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m    using the wordnet corpus reader's built-in _morphy function.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m \u001b[33;03m    :return: The shortest lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     lemmas = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key=\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luiss\\miniconda3\\envs\\NLP\\Lib\\site-packages\\nltk\\stem\\wordnet.py:41\u001b[39m, in \u001b[36mWordNetLemmatizer._morphy\u001b[39m\u001b[34m(self, form, pos, check_exceptions)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33;03m_morphy() is WordNet's _morphy lemmatizer.\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03mIt returns a list of all lemmas found in WordNet.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m \u001b[33;03m['us', 'u']\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wordnet \u001b[38;5;28;01mas\u001b[39;00m wn\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_exceptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\luiss\\miniconda3\\envs\\NLP\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2101\u001b[39m, in \u001b[36mWordNetCorpusReader._morphy\u001b[39m\u001b[34m(self, form, pos, check_exceptions)\u001b[39m\n\u001b[32m   2098\u001b[39m                     seen.add(form)\n\u001b[32m   2099\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m2101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_exceptions \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexceptions\u001b[49m:\n\u001b[32m   2102\u001b[39m     \u001b[38;5;66;03m# 0. Check the exception lists\u001b[39;00m\n\u001b[32m   2103\u001b[39m     forms = exceptions[form]\n\u001b[32m   2104\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2105\u001b[39m     \u001b[38;5;66;03m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#nltk.download()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "pos_tag = nltk.pos_tag(filtered_words)\n",
    "\n",
    "lemmatized_sentences = []\n",
    "for token, tag in pos_tag:\n",
    "    wordnet_pos = get_wordnet_pos(tag) or wordnet.NOUN # if no pos tag found\n",
    "    lemmatized_sentences.append(lemmatizer.lemmatize(filtered_words), pos=wordnet_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24521148",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Create an application with user interface that call a local LLM to solve any problem you choose. Modify the system prompt to better solve the problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
